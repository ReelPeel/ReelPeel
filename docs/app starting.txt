export OLLAMA_CONTEXT_LENGTH=65536

Check models (during runtime):
ollama ps


~/ollama/bin/ollama serve


uvicorn app.main:app --host 0.0.0.0 --port 6006


curl.exe http://im-redstone02.hs-regensburg.de:32311/number

curl.exe -X POST \
  "http://im-redstone02.hs-regensburg.de:32311/process" \
  -H "Content-Type: application/json" \
  -d "{\"url\":\"https://www.instagram.com/reels/DIRM85ZifdM/\",\"mock\":\"true\"}"

WINDOWS FUNKTIONIERT:
curl "http://im-redstone02.hs-regensburg.de:32311/process" `
     -Method Post `
     -Body ( @{ url = "https://www.instagram.com/reels/DIRM85ZifdM/"; mock = "true" } | ConvertTo-Json ) `
     -ContentType "application/json"

Linux Server lokal:
curl -X POST http://localhost:6006/process \
  -H "Content-Type: application/json" \
  -d '{"url":"https://www.instagram.com/reels/C0hXZ3bNAbH/","mock":"false"}'

Testing:
curl -X GET http://localhost:6006/number

https://www.instagram.com/reels/DIRM85ZifdM/  Butter

https://www.instagram.com/reels/DI34PNII2iZ/  Drinking Prep
https://www.instagram.com/reels/DIL8qejowvX/  OPC
https://www.instagram.com/reels/DIlxIbpAjtK/  Beathing
https://www.instagram.com/reels/DJDzRAgNHyv/  Entgiftung




Modelle von Huggingface ziehen: (Public)
ollama run hf.co/QuantFactory/Meditron3-8B-GGUF
ollama run huggingface.co/mradermacher/Meditron3-70B-GGUF

Umbenennen: (Kopieren)
ollama cp huggingface.co/mradermacher/Meditron3-70B-GGUF meditron3-70b

LÃ¶schen:
ollama rm huggingface.co/mradermacher/Meditron3-70B-GGUF

Modelle auflisten:
ollama ls

python -m pip install -r requirements.txt
conda env create -f environment.yml




Multiprocessing:
Bash 1:
export OLLAMA_MULTI_INSTANCE=1
export OLLAMA_MULTI_HOST=127.0.0.1
export OLLAMA_MULTI_BASE_PORT=11434

CUDA_VISIBLE_DEVICES=0 OLLAMA_HOST=127.0.0.1:11434 ollama serve &
CUDA_VISIBLE_DEVICES=1 OLLAMA_HOST=127.0.0.1:11435 ollama serve &
CUDA_VISIBLE_DEVICES=2 OLLAMA_HOST=127.0.0.1:11436 ollama serve &
CUDA_VISIBLE_DEVICES=3 OLLAMA_HOST=127.0.0.1:11437 ollama serve &
wait


Bash 2:
export OLLAMA_MULTI_INSTANCE=1
export OLLAMA_MULTI_HOST=127.0.0.1
export OLLAMA_MULTI_BASE_PORT=11434
uvicorn app.main:app --host 0.0.0.0 --port 6006

Config for Multiprocessing:
{
            "type": "generate_query",  # Step 3 (multi-prompt)
            "settings": {
                "model": STEP_3_MODEL,
                "temperature": BASE_TEMPERATURE,
                "prompt_templates": [
                    {"name": "balanced", "template": PROMPT_TMPL_S3_BALANCED},
                    {"name": "balanced_counter", "template": PROMPT_TMPL_S3_BALANCED_COUNTER},
                    {"name": "specific", "template": PROMPT_TMPL_S3_SPECIFIC},
                    {"name": "specific_counter", "template": PROMPT_TMPL_S3_SPECIFIC_COUNTER},
                    # {"name": "atm_assisted", "template": PROMPT_TMPL_S3_ATM_ASSISTED},
                    # {"name": "atm_assisted_counter", "template": PROMPT_TMPL_S3_ATM_ASSISTED_COUNTER},
                ],
                "parallel": {"enabled": True},
                "prefetch_links": {"enabled": True, "retmax": RETMAX},
            },
        },