# ReelPeel Medical Fact-Checking Pipeline

Extract medical claims from short-form content, retrieve PubMed evidence, and produce a structured verdict with confidence scores.

## What it does

The current pipeline is centered on PubMed-backed medical fact checking:

1. Transcript or mock transcript enters the pipeline.
2. LLM extracts 1-3 medical claims.
3. LLM generates multiple PubMed queries per claim.
4. PubMed IDs are fetched via a local proxy (rate limited).
5. Abstracts are retrieved and summarized.
6. Publication types are converted into evidence weights.
7. Evidence is reranked for relevance.
8. Evidence stance is computed with an NLI model.
9. LLM filters irrelevant evidence.
10. LLM produces a verdict and confidence.
11. Overall truthiness is computed from statement scores.

## Repository layout

```
app/                  # FastAPI app and reel utilities (optional ingestion)
app/main.py           # API entry point
app/reel_utils.py     # Reel download and audio conversion
app/step_1_audio_to_transcript.py
pipeline/             # Core pipeline framework
pipeline/core/        # Orchestrator, models, LLM client, logging
pipeline/steps/       # Extraction, research, scoring, verification
pipeline/test_configs/ # Reference configs and prompt templates
services/             # PubMed proxy service
evaluation/           # Evaluation scripts and datasets
browser-extension/    # UI experiment
final_output.json     # Example output (generated by test.py)
```

## Quick start (pipeline only)

1. Create an environment (choose one):

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Or with conda:

```bash
conda env create -f environment.yml
conda activate factchecker
```

2. Start an OpenAI-compatible LLM endpoint (default is Ollama):

```bash
ollama serve
ollama pull gemma3:12b
```

3. Run the reference pipeline config:

```bash
python test.py
```

This uses `pipeline/test_configs/kai_test.py` and writes `final_output.json` plus a `pipeline_debug_*.log` file.

## Configuration

### Pipeline configuration

The orchestrator consumes a config dictionary. Reference configs live in `pipeline/test_configs/`:

- `pipeline/test_configs/kai_test.py`: end-to-end mock transcript run
- `pipeline/test_configs/multiple_query_config.py`: multi-query variant
- `pipeline/test_configs/raw_eval_config.py`: evaluation variant
- `pipeline/test_configs/test_extraction.py`: extraction-only tests

### Prompt templates

Prompt templates are centralized in `pipeline/test_configs/preprompts.py` and injected per step.

### LLM endpoint settings

All LLM steps use `pipeline/core/llm.py` and are configured per step via `llm_settings`:

```python
{
  "type": "extraction",
  "settings": {
    "model": "gemma3:12b",
    "prompt_template": PROMPT_TMPL_S2,
    "llm_settings": {
      "api_key": "ollama"
    }
  }
}
```

If `llm_settings` is omitted, the default is `http://localhost:11434/v1`.

## PubMed proxy service

The pipeline uses a local proxy (`services/pubmed_proxy.py`) to respect NCBI rate limits. It is auto-started by `pipeline/core/orchestrator.py` via `pipeline/core/service_manager.py`. You can also run it manually:

```bash
python services/pubmed_proxy.py
```

## Optional reel ingestion

The FastAPI app in `app/main.py` and helpers in `app/reel_utils.py` support Instagram reel download and audio extraction. The current pipeline runner is driven by `pipeline/` configs; if you want the API to execute the pipeline end-to-end, wire a `run_pipeline` adapter that calls the orchestrator with your desired config.

## Pipeline diagram

```mermaid
graph TD
    A[Transcript] --> B[Claim extraction]
    B --> C[Query generation]
    C --> D[PubMed fetch]
    D --> E[Abstract fetch + summary]
    E --> F[Pub type weights]
    F --> G[Relevance rerank]
    G --> H[Stance (NLI)]
    H --> I[LLM evidence filter]
    I --> J[LLM verdict]
    J --> K[Overall score]
```

## Notes

- Evidence weights are derived from PubMed publication types.
- Relevance and stance are computed with Transformers models and require torch/transformers.
- The pipeline logs detailed step outputs to `pipeline_debug_<run_id>.log` when debug is enabled.

