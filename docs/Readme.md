# ReelPeel Medical Fact-Checking Pipeline

End-to-end pipeline for medical claim extraction, PubMed evidence retrieval, optional guideline RAG retrieval, and conservative truth scoring.

This project treats the output score as a truth score: 0.0 = claim is false, 1.0 = claim is true.

## What it does (current flow)

1. Input transcript (or mock transcript) enters the pipeline.
2. LLM extracts 1-3 medical claims.
3. LLM generates multiple PubMed queries per claim.
4. PubMed IDs are fetched via a local proxy (rate limited).
5. Abstracts and publication types are retrieved.
6. Publication types are converted into evidence weights.
7. Optional: guideline RAG chunks are retrieved from a local SQLite vector DB.
8. Evidence is reranked for relevance to the claim (title + abstract when available).
9. Evidence stance is computed with an NLI model (claim vs title+abstract).
10. LLM filters irrelevant evidence (PubMed and other non-RAG evidence).
11. LLM produces a verdict and score using PubMed evidence + RAG chunks.
12. Overall truthiness is computed from statement scores.

## Evidence sources

All evidence items share the same union type and are stored in `Statement.evidence`:

- PubMed evidence (`source_type = PubMed`)
  - `pubmed_id`, `url`, `title`, `abstract`, `pub_type`, `weight`, `relevance`, `stance`
- Guideline RAG evidence (`source_type = RAG`)
  - `chunk_id`, `source_path`, `pages`, `abstract`, `score`, `weight`, `relevance`
- Epistemonikos evidence is defined in the schema but not wired into the pipeline yet.

RAG chunks are currently passed to the LLM in a separate `RAG CHUNKS` section.

## Repository layout

```
app/                       # FastAPI app and reel utilities (optional ingestion)
app/main.py                # API entry point
app/reel_utils.py          # Reel download and audio conversion
app/step_1_audio_to_transcript.py
pipeline/                  # Core pipeline framework
pipeline/core/             # Orchestrator, models, LLM client, logging
pipeline/steps/            # Extraction, research, scoring, verification, RAG
pipeline/RAG_vdb/           # Guideline RAG vector DB tools
pipeline/test.py           # Local pipeline runner (uses kai_test config)
pipeline/test_configs/     # Reference configs and prompt templates
services/                  # PubMed proxy service
evaluation/                # Evaluation scripts and datasets
browser-extension/         # UI experiment
final_output.json          # Example output (generated by pipeline/test.py)
```

## Quick start (pipeline only)

1) Create an environment:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Or with conda:

```bash
conda env create -f environment.yml
conda activate factchecker
```

2) Start an OpenAI-compatible LLM endpoint (default is Ollama):

```bash
ollama serve
ollama pull gemma3:27b
```

3) Run the reference pipeline config:

```bash
python pipeline/test.py
```

This uses `pipeline/test_configs/kai_test.py` and writes `final_output.json` plus a `pipeline_debug_*.log` file.

## Guideline RAG setup

The RAG system uses a local SQLite vector DB and SentenceTransformers embeddings.

1) Build a guideline DB:

```bash
python pipeline/RAG_vdb/build_guideline_vdb.py \
  --pdf_dir /path/to/guidelines \
  --db_path pipeline/RAG_vdb/guidelines_vdb.sqlite
```

2) Enable RAG retrieval in a pipeline config (already enabled in `pipeline/test_configs/kai_test.py`):

```python
{
  "type": "retrieve_guideline_facts",
  "settings": {
    "db_path": "pipeline/RAG_vdb/guidelines_vdb.sqlite",
    "top_k": 5,
    "min_score": 0.25,
  },
}
```

3) Optional: use `pipeline/test_configs/rag_test.py` as a minimal RAG config. It can be run by passing that config into the `PipelineOrchestrator` in `pipeline/test.py`.

## Configuration

### Pipeline steps

Key step types (see `pipeline/core/factory.py`):

- `extraction`: transcript -> claims (LLM)
- `generate_query`: claim -> PubMed query (LLM)
- `fetch_links`: PubMed ID lookup
- `summarize_evidence`: abstract + pubtypes fetch (name kept for compatibility)
- `weight_evidence`: weight from pub_type
- `retrieve_guideline_facts`: RAG chunks from SQLite vector DB
- `rerank_evidence`: relevance score using BGE reranker (title+abstract)
- `stance_evidence`: NLI stance using BioLinkBERT MedNLI (title+abstract)
- `filter_evidence`: LLM filter for topicality
- `truthness`: LLM verdict + score
- `scoring`: overall truthiness across statements

### Prompt templates

Prompt templates are centralized in `pipeline/test_configs/preprompts.py` and injected per step.
Step 7 uses `EVIDENCE` plus a separate `RAG CHUNKS` block.

### LLM endpoint settings

Each LLM-based step can override the endpoint with `llm_settings` in its `settings` block:

```python
{
  "type": "extraction",
  "settings": {
    "model": "gemma3:27b",
    "prompt_template": PROMPT_TMPL_S2,
    "llm_settings": {
      "base_url": "http://localhost:11434/v1",
      "api_key": "ollama"
    }
  }
}
```

If `llm_settings` is omitted, the default is `http://localhost:11434/v1`.

### Relevance thresholding and ordering

- `rerank_evidence` supports `min_relevance`. Evidence below this threshold is dropped.
- `truthness` sorts evidence by relevance ascending before sending to the LLM (lowest first, highest last).

## PubMed proxy service

The pipeline uses a local proxy (`services/pubmed_proxy.py`) to respect NCBI rate limits.
It is auto-started by `pipeline/core/orchestrator.py` via `pipeline/core/service_manager.py`.
You can also run it manually:

```bash
python services/pubmed_proxy.py
```

## Output

Each statement includes:

- `verdict`: true | false | uncertain
- `score`: 0.00-1.00 (truth score)
- `evidence`: mixed list of PubMed + RAG evidence

The final JSON is written to `final_output.json` by `pipeline/test.py`.

## Notes and caveats

- Evidence is abstract-only; summaries are no longer generated.
- Reranker and stance models both prepend paper title to the abstract when available.
- RAG chunks are stored as `abstract` in the evidence schema to integrate with the pipeline.
- The verification step passes RAG chunks directly to the LLM without filtering.
- Debug logs are saved as `pipeline_debug_<run_id>.log` when debug is enabled.
